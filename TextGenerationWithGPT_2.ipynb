{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install transformers datasets accelerate\n",
        "\n",
        "# Step 2: Import required modules\n",
        "from datasets import Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, pipeline\n",
        "\n",
        "# Step 3: Load pretrained GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Step 4: Define custom dataset\n",
        "text_data = \"\"\"The dragon’s shadow swept across ancient stone walls.\n",
        "Beneath the silvered moon, the forest whispered forgotten spells.\n",
        "A crown of thorns and stars adorned the exiled queen.\n",
        "In the hidden valley, rivers glowed with liquid crystal light.\n",
        "The sword sang softly, hungry for battle yet to come.\n",
        "Old runes burned brighter as the prophecy drew near.\n",
        "A lone mage traced circles of power into the midnight soil.\n",
        "The castle’s gates stood silent, guarding secrets older than kings.\n",
        "Phoenix ash scattered on the wind, rebirth woven into each ember.\n",
        "Beyond the misty mountains, legends waited to wake once more.\n",
        "\"\"\"\n",
        "\n",
        "text_lines = text_data.strip().split('\\n')\n",
        "\n",
        "# Step 5: Create and tokenize dataset\n",
        "dataset = Dataset.from_dict({\"text\": text_lines})\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Step 6: Set up training configuration\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=100,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Step 7: Fine-tune the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Step 8: Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"gpt2-finetuned\")\n",
        "\n",
        "# Step 9: Generate text using the fine-tuned model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2-finetuned\", tokenizer=tokenizer)\n",
        "prompt = \"Once upon a time\"\n",
        "output = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=100,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "wyAvnGuL9V3n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}