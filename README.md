# PRODIGY_GA_01
Using GPT-2, training a model to generate coherent and contextually relevant text based on a given prompt.

In this project, we fine-tune GPT-2 — a powerful text generation model developed by OpenAI — so it can create new text in a style we choose. By training it on a custom dataset, the model learns to mimic the patterns, tone, and vocabulary of that text.

After fine-tuning, we can give the model a prompt, and it will generate creative and coherent text that continues from that prompt in the same style as our training data. This project shows the full process: loading the pretrained GPT-2 model, preparing custom text, training, saving the new model, and finally using it to produce new, styled text automatically.
